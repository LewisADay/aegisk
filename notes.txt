Current goals:

#Refactor AyncBO to series of classes, for neatness

-Setup selective killing environment
#(When an evaluation finishes, generate candidate location but do not immediately adopt this candidate, first see if any ongoing solution should be killed in favour of this new candidate, if so, kill and adopt the candidate, then seek new candidate (by means of a penalised acquisition function) and repeat until no evaluation should be killed off then adopt the candidate on the originally freed worker.)

#Make selective killing methods as botorch acquisition functions solely for the purposes of optimisation later, not to actually use them as acquisition functions

Selective killing methods to consider:

#Compare EI/ET of candidate to EI/(ET-(t_current-t_start)) of ongoing, if candidate is greater than the ongoing, by some margin delta, adopt the candidate.

Get EI/ET, optimise, then generate the two reciprocal distributions of EI/ET, or indeed any given value function, at the optimal point from the ratio of the means, and find the probability of the candidate having a greater value than the ongoing, if that probability exceeds some threshold, say, 0.8, we adopt the candidate.

#Also we should start calling time, cost, for generality.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Consider using penalised EI rather than straight EI, e.g.
For the scalar method we want EI/EC - what if we use PenalisedEI/EC
where we perform local penalisation on EI, but we remove the considered
ongoing evaluation from the penalisation (otherwise it is unfairly biased)
and we use the full penalised model for the candidate, that way we are
disincentivised from making small design space adjustments when killing

Also, are we going to use the proababilstic method as an acquisition function
as well or only as a killing function? That is do we use it to select the
candidate as well as evaluate the possibility of killing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

